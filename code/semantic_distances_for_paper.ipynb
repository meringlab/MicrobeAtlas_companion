{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all code is written in python (https://www.python.org/) and should be compatible with python 3.10 and later versions.\n",
    "\n",
    "Code author: Matteo Eustachio Peluso (the-puzzler@github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Section Queries LLM For Semantic Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_semantic_clustering(\n",
    "    input_csv_path,\n",
    "    output_json_path,\n",
    "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    num_samples=1_000_000,\n",
    "    sample_size=20,\n",
    "    batch_size=128,\n",
    "    save_interval=10_000,\n",
    "    max_new_tokens=256,\n",
    "    device=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate semantic clusters from word samples using a language model.\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path (str): Path to input CSV file containing words\n",
    "        output_json_path (str): Path to save output JSON file\n",
    "        model_name (str): Hugging Face model name\n",
    "        num_samples (int): Number of random samples to generate\n",
    "        sample_size (int): Number of words per sample\n",
    "        batch_size (int): Processing batch size\n",
    "        save_interval (int): Save outputs every N samples\n",
    "        max_new_tokens (int): Maximum tokens for model generation\n",
    "        device (int): GPU device ID (use -1 for CPU)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup logging\n",
    "    logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model_name, \n",
    "        device=device if device >= 0 else None,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    pipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    word_list = [item for sublist in df.values.tolist() for item in sublist]\n",
    "    \n",
    "    print(f\"Generating {num_samples} random samples...\")\n",
    "    random_samples = [\n",
    "        random.sample(word_list, sample_size) \n",
    "        for _ in range(num_samples)\n",
    "    ]\n",
    "    \n",
    "    # Format prompts\n",
    "    print(\"Formatting prompts...\")\n",
    "    formatted_prompts = []\n",
    "    for sample in random_samples:\n",
    "        sample_text = \", \".join(sample)\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"respond with nothing else other than a numbered list of comma seperated words, no other words, no explanations, nothing else. Return the words exactly as they come, no captialisaion or anything else.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"List of words: {sample_text}. Cluster these random words semantically:\"\n",
    "            }\n",
    "        ]\n",
    "        formatted_prompts.append(prompt)\n",
    "    \n",
    "    # Process in batches\n",
    "    outputs = []\n",
    "    total_batches = len(formatted_prompts) // batch_size + (1 if len(formatted_prompts) % batch_size != 0 else 0)\n",
    "    \n",
    "    for i in tqdm(range(0, len(formatted_prompts), batch_size), total=total_batches, desc=\"Processing batches\"):\n",
    "        batch = formatted_prompts[i:i+batch_size]\n",
    "        batch_outputs = pipe(\n",
    "            batch,\n",
    "            batch_size=batch_size,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        outputs.extend(batch_outputs)\n",
    "        \n",
    "        # Save periodically\n",
    "        if len(outputs) >= save_interval:\n",
    "            _save_outputs(outputs, output_json_path)\n",
    "            outputs = []\n",
    "    \n",
    "    # Save remaining outputs\n",
    "    if outputs:\n",
    "        _save_outputs(outputs, output_json_path)\n",
    "    \n",
    "    print(\"Processing complete\")\n",
    "\n",
    "\n",
    "def _save_outputs(new_outputs, output_path):\n",
    "    \"\"\"Helper function to save outputs, appending to existing file if it exists.\"\"\"\n",
    "    existing_outputs = []\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r') as f:\n",
    "            existing_outputs = json.load(f)\n",
    "    \n",
    "    all_outputs = existing_outputs + new_outputs\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(all_outputs, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(all_outputs)} total outputs to {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    run_semantic_clustering(\n",
    "        input_csv_path=\"../data/input_words.csv\",\n",
    "        output_json_path=\"semantic_clusters.json\",\n",
    "        num_samples=1_000_000,\n",
    "        sample_size=20,\n",
    "        batch_size=128,\n",
    "        save_interval=10_000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Section Calculates Semantic Distances Based on Cluster Membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is MP Code, should be called from a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "def similarity_ratio(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def find_best_matching_input_word(output_word, input_words, threshold=0.85):\n",
    "    best_match = max(((word, similarity_ratio(output_word, word)) \n",
    "                     for word in input_words), \n",
    "                    key=lambda x: x[1])\n",
    "    return best_match[0] if best_match[1] >= threshold else None\n",
    "\n",
    "def process_cluster_sample(sample):\n",
    "    try:\n",
    "        # Extract input words\n",
    "        input_content = next(msg[\"content\"] for msg in sample[0][\"generated_text\"] \n",
    "                           if msg[\"role\"] == \"user\")\n",
    "        words_section = input_content[input_content.find(\"words: \") + 7:].split(\".\")[0]\n",
    "        input_words = [w.strip() for w in words_section.split(\",\")]\n",
    "        \n",
    "        # Extract and process clusters\n",
    "        output_content = next(msg[\"content\"] for msg in sample[0][\"generated_text\"] \n",
    "                            if msg[\"role\"] == \"assistant\")\n",
    "        \n",
    "        clusters = []\n",
    "        used_words = set()\n",
    "        \n",
    "        for line in output_content.split('\\n'):\n",
    "            if not line.strip() or not any(c.isdigit() for c in line):\n",
    "                continue\n",
    "                \n",
    "            words_part = line.split('.', 1)[1].strip() if '.' in line else line.lstrip(\"0123456789 .\")\n",
    "            output_words = [w.strip() for w in words_part.split(\",\")]\n",
    "            \n",
    "            valid_words = []\n",
    "            cluster_used_words = set()\n",
    "            \n",
    "            for output_word in output_words:\n",
    "                matched_word = find_best_matching_input_word(output_word, input_words)\n",
    "                if matched_word and matched_word not in cluster_used_words:\n",
    "                    valid_words.append(matched_word)\n",
    "                    cluster_used_words.add(matched_word)\n",
    "            \n",
    "            if valid_words:\n",
    "                clusters.append(valid_words)\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_all_clusters_parallel(sample_data, num_processes=None, verbose=False):\n",
    "   with Pool(processes=num_processes) as pool:\n",
    "       if verbose:\n",
    "           results = list(tqdm.tqdm(\n",
    "               pool.imap(process_cluster_sample, sample_data),\n",
    "               total=len(sample_data),\n",
    "               desc=\"Processing samples\"\n",
    "           ))\n",
    "       else:\n",
    "           results = list(pool.imap(process_cluster_sample, sample_data))\n",
    "   \n",
    "   successful = sum(1 for r in results if r)\n",
    "   if verbose:\n",
    "       print(f\"\\nSuccessfully processed {successful} out of {len(sample_data)} samples\")\n",
    "   return [r for r in results if r]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code takes the parsed data from the MP code and calculates the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_term_relationships(contexts):\n",
    "    \"\"\"\n",
    "    Compute relationship scores between terms based on their cluster co-occurrences.\n",
    "    \n",
    "    Args:\n",
    "        contexts (list): List of contexts, where each context is a list of clusters,\n",
    "                        and each cluster is a collection of terms.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - 'relationship_matrix': numpy array of relationship scores\n",
    "            - 'term_to_idx': mapping of terms to matrix indices\n",
    "            - 'all_terms': list of all unique terms\n",
    "    \"\"\"\n",
    "    # Collect all unique terms\n",
    "    all_terms = set()\n",
    "    for context in contexts:\n",
    "        for cluster in context:\n",
    "            all_terms.update(cluster)\n",
    "    all_terms = list(all_terms)\n",
    "    term_to_idx = {term: idx for idx, term in enumerate(all_terms)}\n",
    "\n",
    "    # Initialize matrices\n",
    "    n_terms = len(all_terms)\n",
    "    same_cluster_count = np.zeros((n_terms, n_terms))\n",
    "    same_context_count = np.zeros((n_terms, n_terms))\n",
    "    different_cluster_count = np.zeros((n_terms, n_terms))\n",
    "\n",
    "    # Count co-occurrences\n",
    "    for context in tqdm(contexts, desc=\"Processing contexts\", unit=\"context\"):\n",
    "        terms_in_context = set()\n",
    "        cluster_membership = {}\n",
    "\n",
    "        # First pass: collect terms and their cluster assignments\n",
    "        for cluster_idx, cluster in enumerate(context):\n",
    "            for term in cluster:\n",
    "                terms_in_context.add(term)\n",
    "                cluster_membership[term] = cluster_idx\n",
    "\n",
    "        # Second pass: update counts\n",
    "        for term1 in terms_in_context:\n",
    "            idx1 = term_to_idx[term1]\n",
    "            for term2 in terms_in_context:\n",
    "                if term1 != term2:\n",
    "                    idx2 = term_to_idx[term2]\n",
    "                    same_context_count[idx1][idx2] += 1\n",
    "\n",
    "                    if cluster_membership[term1] == cluster_membership[term2]:\n",
    "                        same_cluster_count[idx1][idx2] += 1\n",
    "                    else:\n",
    "                        different_cluster_count[idx1][idx2] += 1\n",
    "\n",
    "    # Calculate relationship scores\n",
    "    relationship_matrix = np.zeros((n_terms, n_terms))\n",
    "    for i in tqdm(range(n_terms), desc=\"Computing relationship matrix\", unit=\"term\"):\n",
    "        for j in range(n_terms):\n",
    "            if i != j and same_context_count[i][j] > 0:\n",
    "                score = (same_cluster_count[i][j] - different_cluster_count[i][j]) / same_context_count[i][j]\n",
    "                relationship_matrix[i][j] = score\n",
    "\n",
    "    return {\n",
    "        'relationship_matrix': relationship_matrix,\n",
    "        'term_to_idx': term_to_idx,\n",
    "        'all_terms': all_terms\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code puts it all together, calls the multiprocessing parsing and cleaning step, then calculates distances between terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import h5py\n",
    "\n",
    "def analyze_clustering_results(\n",
    "   json_file_path,\n",
    "   num_processes=None\n",
    "):\n",
    "   \"\"\"\n",
    "   Analyze clustering results from JSON file and compute term relationships.\n",
    "   \n",
    "   Args:\n",
    "       json_file_path (str): Path to the JSON file containing clustering results\n",
    "       num_processes (int): Number of processes for parallel processing. \n",
    "                          If None, uses (CPU cores - 1)\n",
    "   \n",
    "   Returns:\n",
    "       dict: Dictionary containing:\n",
    "           - relationship_matrix: Matrix of term relationships\n",
    "           - term_to_idx: Mapping from terms to matrix indices\n",
    "           - all_terms: List of all unique terms\n",
    "   \"\"\"\n",
    "   import json\n",
    "   import multiprocessing\n",
    "   \n",
    "   # Load data\n",
    "   with open(json_file_path, 'r') as f:\n",
    "       sample_data = json.load(f)\n",
    "   \n",
    "   # Set number of processes\n",
    "   if num_processes is None:\n",
    "       num_cores = multiprocessing.cpu_count()\n",
    "       num_processes = max(1, num_cores - 1)\n",
    "   \n",
    "   # Process clusters and compute relationships\n",
    "   all_cluster_results = process_all_clusters_parallel(\n",
    "       sample_data, \n",
    "       num_processes=num_processes\n",
    "   )\n",
    "   \n",
    "   results = compute_term_relationships(all_cluster_results)\n",
    "   \n",
    "   return {\n",
    "       'relationship_matrix': results['relationship_matrix'],\n",
    "       'term_to_idx': results['term_to_idx'],\n",
    "       'all_terms': results['all_terms']\n",
    "   }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   results = analyze_clustering_results('../data/cluster_words3.json')\n",
    "   relationship_matrix = results['relationship_matrix']\n",
    "   term_to_idx = results['term_to_idx']\n",
    "   all_terms = results['all_terms']\n",
    "   \n",
    "   print(f\"Loaded {len(all_terms)} unique terms\")\n",
    "   print(f\"Relationship matrix shape: {relationship_matrix.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mapseq)",
   "language": "python",
   "name": "mapseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
